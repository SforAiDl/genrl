

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Vanilla Policy Gradient &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Advantage Actor Critic" href="A2C.html" />
    <link rel="prev" title="Deep Reinforcement Learning Background" href="Background.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../bandit/index.html">Bandit Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classical/index.html">Classical</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Deep RL Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="Background.html">Deep Reinforcement Learning Background</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Vanilla Policy Gradient</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#objective">Objective</a></li>
<li class="toctree-l4"><a class="reference internal" href="#algorithm-details">Algorithm Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-through-the-api">Training through the API</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="A2C.html">Advantage Actor Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="PPO.html">Proximal Policy Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="DQN.html">Deep Q-Networks (DQN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="Double_DQN.html">Double Deep Q-Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="Dueling_DQN.html">Dueling Deep Q-Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="NoisyNet_DQN.html">Deep Q Networks with Noisy Nets</a></li>
<li class="toctree-l3"><a class="reference internal" href="Prioritized_DQN.html">Prioritized Deep Q-Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="DDPG.html">Deep Deterministic Policy Gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="TD3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l3"><a class="reference internal" href="SAC.html">Soft Actor-Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="Categorical_DQN.html">Categorical Deep Q-Networks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20Custom%20Policies.html">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20shared%20parameters%20in%20actor%20critic%20agents.html">Using Shared Parameters in Actor Critic Agents in GenRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Saving%20and%20loading.html">Saving and Loading Weights and Hyperparameters with GenRL</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
          <li><a href="index.html">Deep RL Tutorials</a> &raquo;</li>
        
      <li>Vanilla Policy Gradient</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/usage/tutorials/Deep/VPG.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vanilla-policy-gradient">
<h1>Vanilla Policy Gradient<a class="headerlink" href="#vanilla-policy-gradient" title="Permalink to this headline">¶</a></h1>
<p>For background on Deep RL, its core definitions and problem formulations refer to <span class="xref std std-ref">Deep RL Background</span></p>
<div class="section" id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h2>
<p>The objective is to choose/learn a policy that will maximize a cumulative function of rewards received at each step, typically the discounted reward over a potential infinite horizon. We formulate this cumulative function as</p>
<div class="math notranslate nohighlight">
\[E\left[{\sum_{t=0}^{\infty}{\gamma^{t} r_{t}}}\right]\]</div>
<p>where we choose the action <span class="math notranslate nohighlight">\(a_{t} = \pi_{\theta}(s_{t})\)</span>.</p>
</div>
<div class="section" id="algorithm-details">
<h2>Algorithm Details<a class="headerlink" href="#algorithm-details" title="Permalink to this headline">¶</a></h2>
<div class="section" id="collect-experience">
<h3>Collect Experience<a class="headerlink" href="#collect-experience" title="Permalink to this headline">¶</a></h3>
<p>To make our agent learn, we first need to collect some experience in an online fashion. For this we make use of the <code class="docutils literal notranslate"><span class="pre">collect_rollouts</span></code> method. This method is defined in the <code class="docutils literal notranslate"><span class="pre">OnPolicyAgent</span></code> Base Class.</p>
<p>For updation, we would need to compute advantages from this experience. So, we store our experience in a Rollout Buffer.
Action Selection
—————-</p>
<p>Note: We sample a <strong>stochastic action</strong> from the distribution on the action space by providing <code class="docutils literal notranslate"><span class="pre">False</span></code> as an argument to <code class="docutils literal notranslate"><span class="pre">select_action</span></code>.</p>
<p>For practical purposes we would assume that we are working with a finite horizon MDP.</p>
</div>
<div class="section" id="update-equations">
<h3>Update Equations<a class="headerlink" href="#update-equations" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> denote a policy with parameters <span class="math notranslate nohighlight">\(\theta\)</span>, and <span class="math notranslate nohighlight">\(J(\pi_{\theta})\)</span> denote the expected finite-horizon undiscounted return of the policy.</p>
<p>At each update timestep, we get value and log probabilities:</p>
<p>Now, that we have the log probabilities we calculate the gradient of <span class="math notranslate nohighlight">\(J(\pi_{\theta})\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta} J(\pi_{\theta}) = E_{\tau \sim \pi_{\theta}}\left[{
    \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)
    }\right],\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau\)</span> is the trajectory.</p>
<p>We then update the policy parameters via stochastic gradient ascent:</p>
<div class="math notranslate nohighlight">
\[\theta_{k+1} = \theta_k + \alpha \nabla_{\theta} J(\pi_{\theta_k})\]</div>
<p>The key idea underlying vanilla policy gradients is to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to lower return, until you arrive at the optimal policy.</p>
</div>
</div>
<div class="section" id="training-through-the-api">
<h2>Training through the API<a class="headerlink" href="#training-through-the-api" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="kn">from</span> <span class="nn">genrl.agents</span> <span class="kn">import</span> <span class="n">VPG</span>
<span class="kn">from</span> <span class="nn">genrl.trainers</span> <span class="kn">import</span> <span class="n">OnPolicyTrainer</span>
<span class="kn">from</span> <span class="nn">genrl.environments</span> <span class="kn">import</span> <span class="n">VectorEnv</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">VectorEnv</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">VPG</span><span class="p">(</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">OnPolicyTrainer</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">log_mode</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;stdout&#39;</span><span class="p">])</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>timestep         Episode          loss             mean_reward
<span class="m">0</span>                <span class="m">0</span>                <span class="m">9</span>.1853           <span class="m">22</span>.3825
<span class="m">20480</span>            <span class="m">10</span>               <span class="m">24</span>.5517          <span class="m">80</span>.3137
<span class="m">40960</span>            <span class="m">20</span>               <span class="m">24</span>.4992          <span class="m">117</span>.7011
<span class="m">61440</span>            <span class="m">30</span>               <span class="m">22</span>.578           <span class="m">121</span>.543
<span class="m">81920</span>            <span class="m">40</span>               <span class="m">20</span>.423           <span class="m">114</span>.7339
<span class="m">102400</span>           <span class="m">50</span>               <span class="m">21</span>.7225          <span class="m">128</span>.4013
<span class="m">122880</span>           <span class="m">60</span>               <span class="m">21</span>.0566          <span class="m">116</span>.034
<span class="m">143360</span>           <span class="m">70</span>               <span class="m">21</span>.628           <span class="m">115</span>.0562
<span class="m">163840</span>           <span class="m">80</span>               <span class="m">23</span>.1384          <span class="m">133</span>.4202
<span class="m">184320</span>           <span class="m">90</span>               <span class="m">23</span>.2824          <span class="m">133</span>.4202
<span class="m">204800</span>           <span class="m">100</span>              <span class="m">26</span>.3477          <span class="m">147</span>.87
<span class="m">225280</span>           <span class="m">110</span>              <span class="m">26</span>.7198          <span class="m">139</span>.7952
<span class="m">245760</span>           <span class="m">120</span>              <span class="m">30</span>.0402          <span class="m">184</span>.5045
<span class="m">266240</span>           <span class="m">130</span>              <span class="m">30</span>.293           <span class="m">178</span>.8646
<span class="m">286720</span>           <span class="m">140</span>              <span class="m">29</span>.4063          <span class="m">162</span>.5397
<span class="m">307200</span>           <span class="m">150</span>              <span class="m">30</span>.9759          <span class="m">183</span>.6771
<span class="m">327680</span>           <span class="m">160</span>              <span class="m">30</span>.6517          <span class="m">186</span>.1818
<span class="m">348160</span>           <span class="m">170</span>              <span class="m">31</span>.7742          <span class="m">184</span>.5045
<span class="m">368640</span>           <span class="m">180</span>              <span class="m">30</span>.4608          <span class="m">186</span>.1818
<span class="m">389120</span>           <span class="m">190</span>              <span class="m">30</span>.2635          <span class="m">186</span>.1818
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="A2C.html" class="btn btn-neutral float-right" title="Advantage Actor Critic" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Background.html" class="btn btn-neutral float-left" title="Deep Reinforcement Learning Background" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>