

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Custom Policy Networks &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Using A2C" href="Using%20A2C.html" />
    <link rel="prev" title="Categorical Deep Q-Networks" href="Deep/Categorical_DQN.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="bandit/index.html">Bandit Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="Classical/index.html">Classical</a></li>
<li class="toctree-l2"><a class="reference internal" href="Deep/index.html">Deep RL Tutorials</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="Using%20shared%20parameters%20in%20actor%20critic%20agents.html">Using Shared Parameters in Actor Critic Agents in GenRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Saving%20and%20loading.html">Saving and Loading Weights and Hyperparameters with GenRL</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Tutorials</a> &raquo;</li>
        
      <li>Custom Policy Networks</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/usage/tutorials/Using Custom Policies.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="custom-policy-networks">
<h1>Custom Policy Networks<a class="headerlink" href="#custom-policy-networks" title="Permalink to this headline">¶</a></h1>
<p>GenRL provides default policies for images (CNNPolicy) and for other types of inputs(MlpPolicy).
Sometimes, these default policies may be insuffiecient for your problem, or you may want more control over the policy definition, and hence require a custom policy.</p>
<p>The following code tutorial runs through the steps to use a custom policy depending on your problem.</p>
<p>Import the required libraries (eg. torch, torch.nn) and from GenRL, the algorithm (eg VPG), the trainer (eg. OnPolicyTrainer), the policy to be modified (eg. MlpPolicy)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The necessary imports</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">from</span> <span class="nn">genrl.agents</span> <span class="kn">import</span> <span class="n">VPG</span>
<span class="kn">from</span> <span class="nn">genrl.core.policies</span> <span class="kn">import</span> <span class="n">MlpPolicy</span>
<span class="kn">from</span> <span class="nn">genrl.environments</span> <span class="kn">import</span> <span class="n">VectorEnv</span>
<span class="kn">from</span> <span class="nn">genrl.trainers</span> <span class="kn">import</span> <span class="n">OnPolicyTrainer</span>
</pre></div>
</div>
<p>Then define a <code class="docutils literal notranslate"><span class="pre">custom_policy</span></code> class that derives from the policy to be modified (in this case, the <code class="docutils literal notranslate"><span class="pre">MlpPolicy</span></code>)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a custom MLP Policy</span>
<span class="k">class</span> <span class="nc">custom_policy</span><span class="p">(</span><span class="n">MlpPolicy</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span> <span class="o">=</span> <span class="n">state_dim</span>
</pre></div>
</div>
<p>The above class modifies the MlpPolicy to have the desired number of hidden layers in the MLP Neural network that parametrizes the policy.
This is done by passing the variable hidden explicitly (default<code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">=</span> <span class="pre">(32,</span> <span class="pre">32)</span></code>). The <code class="docutils literal notranslate"><span class="pre">state_dim</span></code> and <code class="docutils literal notranslate"><span class="pre">action_dim</span></code> variables stand for the dimensions of the state_space and the action_space, and are required to construct the neural network with the proper input and output shapes for your policy, given the environment.</p>
<p>In some cases, you may also want to redefine the policy used completely and not just customize and existing policy. This can be done by creating a new custom policy class that inhierits the BasePolicy class.
The BasePolicy class is a basic implementation of a general policy, with a <code class="docutils literal notranslate"><span class="pre">forward</span></code> and a <code class="docutils literal notranslate"><span class="pre">get_action</span></code> method. The forward method maps the input state to the action probabilities,
and the <code class="docutils literal notranslate"><span class="pre">get_action</span></code> method selects an action from the given action probabilities (for both continuous and discrete action_spaces)</p>
<p>Say you want to parametrize your policy by a Neural Network containing LSTM layers followed my MLP layers. This can be done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a custom LSTM policy from the BasePolicy class</span>
<span class="k">class</span> <span class="nc">custom_policy</span><span class="p">(</span><span class="n">BasePolicy</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layer_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">custom_policy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span>
                                            <span class="n">action_dim</span><span class="p">,</span>
                                            <span class="n">hidden</span><span class="p">,</span>
                                            <span class="n">discrete</span><span class="p">,</span>
                                            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span> <span class="o">=</span> <span class="n">state_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_size</span> <span class="o">=</span> <span class="n">layer_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">layer_size</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">([</span><span class="n">layer_size</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">action_dim</span><span class="p">],</span>
                      <span class="n">sac</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sac</span><span class="p">)</span>  <span class="c1"># the mlp layers</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_size</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
<p>Finally, it’s time to train the custom policy. Define the environment to be trained on (<code class="docutils literal notranslate"><span class="pre">CartPole-v0</span></code> in this case), and the <code class="docutils literal notranslate"><span class="pre">state_dim</span></code> and <code class="docutils literal notranslate"><span class="pre">action_dim</span></code> variables.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize an environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">VectorEnv</span><span class="p">(</span><span class="s2">&quot;CartPole-v0&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize the custom Policy</span>
<span class="n">state_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">action_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">custom_policy</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="o">=</span><span class="n">action_dim</span><span class="p">,</span>
                        <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
</pre></div>
</div>
<p>Then the algorithm is initialised with the custom policy defined, and the OnPolicyTrainer trains in with logging for better inference.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span> <span class="o">=</span> <span class="n">VPG</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>

<span class="c1"># Initialize the trainer and start training </span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">OnPolicyTrainer</span><span class="p">(</span><span class="n">algo</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">log_mode</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;csv&quot;</span><span class="p">],</span>
                          <span class="n">logdir</span><span class="o">=</span><span class="s2">&quot;./logs&quot;</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Using%20A2C.html" class="btn btn-neutral float-right" title="Using A2C" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Deep/Categorical_DQN.html" class="btn btn-neutral float-left" title="Categorical Deep Q-Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>