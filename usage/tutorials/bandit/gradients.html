

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gradients &mdash; GenRL 0.1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/genrl_cropped.png"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Linear Posterior Inference" href="linpos.html" />
    <link rel="prev" title="Bayesian" href="bayesian.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../../index.html">
          

          
            
            <img src="../../../_static/genrl.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Bandit Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="bandit_overview.html">Multi Armed Bandit Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="contextual_overview.html">Contextual Bandits Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="ucb.html">UCB</a></li>
<li class="toctree-l3"><a class="reference internal" href="thompson_sampling.html">Thompson Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesian.html">Bayesian</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Gradients</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-gradient-method-on-a-bernoulli-multi-armed-bandit">Using Gradient Method on a Bernoulli Multi-Armed Bandit</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="linpos.html">Linear Posterior Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="variational.html">Variational Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="bootstrap.html">Bootstrap</a></li>
<li class="toctree-l3"><a class="reference internal" href="noise.html">Parameter Noise Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="adding_data_bandit.html">Adding a new Data Bandit</a></li>
<li class="toctree-l3"><a class="reference internal" href="adding_dcb_agent.html">Adding a new Deep Contextual Bandit Agent</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Classical/index.html">Classical</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Deep/index.html">Deep RL Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20Custom%20Policies.html">Custom Policy Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20A2C.html">Using A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20shared%20parameters%20in%20actor%20critic%20agents.html">Using Shared Parameters in Actor Critic Agents in GenRL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../using_vpg.html">Vanilla Policy Gradient (VPG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Saving%20and%20loading.html">Saving and Loading Weights and Hyperparameters with GenRL</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/agents/index.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/core/index.html">Core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/utils/index.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/trainers/index.html">Trainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/common/index.html">Common</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">GenRL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
          <li><a href="index.html">Bandit Tutorials</a> &raquo;</li>
        
      <li>Gradients</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/usage/tutorials/bandit/gradients.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="gradients">
<h1>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">¶</a></h1>
<div class="section" id="using-gradient-method-on-a-bernoulli-multi-armed-bandit">
<h2>Using Gradient Method on a Bernoulli Multi-Armed Bandit<a class="headerlink" href="#using-gradient-method-on-a-bernoulli-multi-armed-bandit" title="Permalink to this headline">¶</a></h2>
<p>For an introduction to Multi Armed Bandits, refer to <a class="reference internal" href="bandit_overview.html#bandit-overview"><span class="std std-ref">Multi Armed Bandit Overview</span></a></p>
<p>This method is different compared to others. In other methods, we
explicity attempt to estimate the ‘value’ of taking an action (its
quality) whereas in this method we approach the problem in a different
way. Here, instead of estimating how good an action is through its
quality, we only care about its preference of being selected compared to
other actions. We denote this preference by <span class="math notranslate nohighlight">\(H_t(a)\)</span>. The larger
the preference of an action ‘a’, more are the chances of it being
selected, but this preference has no interpretation in terms of the
reward for that action. Only the relative preference is important.</p>
<p>The action probabilites are related to these action preferences
<span class="math notranslate nohighlight">\(H_t(a)\)</span> by a softmax function. The probability of taking action
<span class="math notranslate nohighlight">\(a_j\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[P(a_j) = \frac{e^{H_t(a_j)}}{\sum_{i=1}^A e^{H_t(a_i)}} = \pi_t(a_j)\]</div>
<p>where, A is the total number of actions and <span class="math notranslate nohighlight">\(\pi_t(a)\)</span> is the
probability of taking action ‘a’ at timestep ‘t’.</p>
<p>We initialise the preferences for all the actions to be 0, meaning
<span class="math notranslate nohighlight">\(\pi_t(a) = \frac{1}{A}\)</span> for all actions.</p>
<p>After computing <span class="math notranslate nohighlight">\(\pi_t(a)\)</span> for all actions at each timestep, the
action is sampled using this probability. Then that action is performed
and based on the reward we get, we update our preferences.</p>
<p>The update rule bacially performs stochastic gradient ascent:</p>
<p><span class="math notranslate nohighlight">\(H_{t+1}(a_t) = H_t(a_t) + \alpha (R_t - \bar{R_t})(1-\pi_t(a_t))\)</span>,
for <span class="math notranslate nohighlight">\(a_t\)</span>: action taken at time ‘t’</p>
<p><span class="math notranslate nohighlight">\(H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R_t})(\pi_t(a))\)</span> for
rest of the actions</p>
<p>where, <span class="math notranslate nohighlight">\(\alpha\)</span> is the step size, <span class="math notranslate nohighlight">\(R_t\)</span> is the reward
obtained at time ‘t’ and <span class="math notranslate nohighlight">\(\bar{R_t}\)</span> is the mean reward obtained
upto time t. If current reward is larger than the mean reward, we
increase our preference for that action taken at time ‘t’. If it is
lower than the mean reward, we decrease our preference for that action.
The preferences for the rest of the actions are updated in the opposite
direction.</p>
<p>For a more detailed mathematical analysis and derivation of the update
rule, refer to chapter 2 of Sutton &amp; Barto.</p>
<p>Code to use the Gradient method on a Bernoulli Multi-Armed Bandit:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">genrl.bandit</span> <span class="kn">import</span> <span class="n">BernoulliMAB</span><span class="p">,</span> <span class="n">GradientMABAgent</span><span class="p">,</span> <span class="n">MABTrainer</span>

<span class="n">bandits</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">arms</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">reward_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">bandits</span><span class="p">,</span> <span class="n">arms</span><span class="p">))</span>
<span class="n">bandit</span> <span class="o">=</span> <span class="n">BernoulliMAB</span><span class="p">(</span><span class="n">bandits</span><span class="p">,</span> <span class="n">arms</span><span class="p">,</span> <span class="n">reward_probs</span><span class="p">,</span> <span class="n">context_type</span><span class="o">=</span><span class="s2">&quot;int&quot;</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">GradientMABAgent</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">temp</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">MABTrainer</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">bandit</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">timesteps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<p>More details can be found in the docs for
<a class="reference external" href="../../../api/bandit/genrl.core.bandit.html#genrl.core.bandit.bernoulli_mab.BernoulliMAB">BernoulliMAB</a>,
<a class="reference external" href="../../../api/bandit/genrl.agents.bandits.multiarmed.html#module-genrl.agents.bandits.multiarmed.gradient">BayesianUCBMABAgent</a>
and
<a class="reference external" href="../../../api/common/bandit.html#module-genrl.bandit.trainer">MABTrainer</a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="linpos.html" class="btn btn-neutral float-right" title="Linear Posterior Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="bayesian.html" class="btn btn-neutral float-left" title="Bayesian" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Society for Artificial Intelligence and Deep Learning (SAiDL)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>